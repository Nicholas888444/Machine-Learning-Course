{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Neural Network with a Data Loader\n",
    "In PyTorch, a **data loader** is a utility that helps in efficiently loading and iterating over data during the training or evaluation of a machine learning model. It is particularly useful when working with large datasets that cannot fit entirely into memory.\n",
    "\n",
    "In this tutorial, you will learn how to create a data loader to load batches of data from a csv file. This will be essential if your csv file is too large to be loaded into your computer's memory at once.\n",
    "\n",
    "We will still use the Pima Indians Diabetes dataset, which can be found at https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/yangliuiuk/data/main/diabetes.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Use oversampling to address class inbalance issue\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X, y = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# After standardized, X_train and X_test will be converted from pandas data frame into numpy arrays. To make the type of X data the same as y data (pandas series), we convert X data back to pandas data frame.\n",
    "# The purpose of this step is to make the code syntax to convert X and y data to PyTorch tensors to be consistent.\n",
    "\n",
    "X_train = pd.DataFrame(X_train) \n",
    "X_test = pd.DataFrame(X_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 8]) torch.Size([800]) torch.Size([200, 8]) torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32) # X_train.values will convert X_train from a pandas dataframe into an numpy array, which is required as the input type for torch.tensor().\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long) \n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long) \n",
    "\n",
    "print(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type for y_train_tensor is torch.long, which is an integer representing the class label. \n",
    "\n",
    "For instance, if we have two classes, each y value can be either 0 or 1. If we have three classes, each y value can be either 0, 1, or 2. \n",
    "\n",
    "This representation of y is more flexible for multple-class classification.\n",
    "\n",
    "We print out the shape attribute for each tensor to ensure each tensor is in the correct shape.\n",
    "\n",
    "In this example, we no only use unsqueeze(1) on y tensors. Also, when constructing neural network, we don't need to add a sigmoid function on the final output layer to output a class probability.\n",
    "\n",
    "Instead, the output layer will just be raw class weights, also called **logits**. Logits refers to the raw, unnormalized outputs of a neural network before any activation function is applied to them.\n",
    "\n",
    "For instance, if the class weights are [100, 50], it means there are higher chance to be class 0 then class 1.\n",
    "Note that it doesn't mean there is 2/3 chance to be class 0 and 1/3 chance to be class 1. This is because raw class weights produced by a linear layer can be negative numbers. \n",
    "\n",
    "Their is one more step to convert raw class weights into class probabilities, which is the **softmax**. It just applies the exponential function on each raw class weight to make it possitive. Then compute class probability distribution based on positve class weights.\n",
    "\n",
    "For instance, the raw class weights for class 0 and class 1 are [-1, 0.5]. Then, the positive class weights will be [e^-1, e^0.5] = [0.37, 1.65]. Then, the class probabilities are:\n",
    "\n",
    "probability of class 0 = 0.37 / (0.37 + 1.65) = 0.18\n",
    "\n",
    "probability of class 1 = 1.65 / (0.37 + 1.65) = 0.82\n",
    "\n",
    "So the class probability distribution is [0.18, 0.82]\n",
    "\n",
    "Suppose the real class label is \"0\". The output of a neural network is the raw class weights [-1, 0.5]. How to compute the loss between them? \n",
    "Fortunately, we can skip the step of computing class probabily distribution from raw neural network outputs. This step can be done by PyTorch loss functions.\n",
    "\n",
    "We can just use the **CrossEntropyLoss** loss function. Note that it is different from **BinaryCrossEntrophyLoss**, although they are highly correlated. CrossEntropyLoss loss is more flexible if the data has more than two classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders\n",
    "\n",
    "In this step, we will create data loaders for the training and testing set. The data loader can randomly draw a batch (a small subset) of data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset objects for train and test data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader objects for train and test datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the neural network instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = X_train_tensor.shape[1] * 2 # The size of hidden layer is arbitrarily chosen and can be tuned.\n",
    "num_classes = 2 # Number of classes in your multi-class classification problem\n",
    "\n",
    "# Instantiate the neural network model\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # CrossEntroyLoss \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.5433\n",
      "Epoch [10/200], Loss: 0.6416\n",
      "Epoch [10/200], Loss: 0.5771\n",
      "Epoch [10/200], Loss: 0.6109\n",
      "Epoch [10/200], Loss: 0.6133\n",
      "Epoch [10/200], Loss: 0.6329\n",
      "Epoch [10/200], Loss: 0.5947\n",
      "Epoch [10/200], Loss: 0.5496\n",
      "Epoch [10/200], Loss: 0.5785\n",
      "Epoch [10/200], Loss: 0.5699\n",
      "Epoch [10/200], Loss: 0.5853\n",
      "Epoch [10/200], Loss: 0.6293\n",
      "Epoch [10/200], Loss: 0.6253\n",
      "Epoch [20/200], Loss: 0.5859\n",
      "Epoch [20/200], Loss: 0.6455\n",
      "Epoch [20/200], Loss: 0.5864\n",
      "Epoch [20/200], Loss: 0.4444\n",
      "Epoch [20/200], Loss: 0.4306\n",
      "Epoch [20/200], Loss: 0.4291\n",
      "Epoch [20/200], Loss: 0.5313\n",
      "Epoch [20/200], Loss: 0.4595\n",
      "Epoch [20/200], Loss: 0.5083\n",
      "Epoch [20/200], Loss: 0.5090\n",
      "Epoch [20/200], Loss: 0.4595\n",
      "Epoch [20/200], Loss: 0.5276\n",
      "Epoch [20/200], Loss: 0.6587\n",
      "Epoch [30/200], Loss: 0.5736\n",
      "Epoch [30/200], Loss: 0.4036\n",
      "Epoch [30/200], Loss: 0.3784\n",
      "Epoch [30/200], Loss: 0.4292\n",
      "Epoch [30/200], Loss: 0.5113\n",
      "Epoch [30/200], Loss: 0.5404\n",
      "Epoch [30/200], Loss: 0.4333\n",
      "Epoch [30/200], Loss: 0.4806\n",
      "Epoch [30/200], Loss: 0.4965\n",
      "Epoch [30/200], Loss: 0.5686\n",
      "Epoch [30/200], Loss: 0.5633\n",
      "Epoch [30/200], Loss: 0.4608\n",
      "Epoch [30/200], Loss: 0.5907\n",
      "Epoch [40/200], Loss: 0.5326\n",
      "Epoch [40/200], Loss: 0.4572\n",
      "Epoch [40/200], Loss: 0.4564\n",
      "Epoch [40/200], Loss: 0.5665\n",
      "Epoch [40/200], Loss: 0.4733\n",
      "Epoch [40/200], Loss: 0.4718\n",
      "Epoch [40/200], Loss: 0.4004\n",
      "Epoch [40/200], Loss: 0.5062\n",
      "Epoch [40/200], Loss: 0.4945\n",
      "Epoch [40/200], Loss: 0.5290\n",
      "Epoch [40/200], Loss: 0.4132\n",
      "Epoch [40/200], Loss: 0.4871\n",
      "Epoch [40/200], Loss: 0.4171\n",
      "Epoch [50/200], Loss: 0.5075\n",
      "Epoch [50/200], Loss: 0.4415\n",
      "Epoch [50/200], Loss: 0.4608\n",
      "Epoch [50/200], Loss: 0.4416\n",
      "Epoch [50/200], Loss: 0.3610\n",
      "Epoch [50/200], Loss: 0.5513\n",
      "Epoch [50/200], Loss: 0.5802\n",
      "Epoch [50/200], Loss: 0.4494\n",
      "Epoch [50/200], Loss: 0.4897\n",
      "Epoch [50/200], Loss: 0.6339\n",
      "Epoch [50/200], Loss: 0.4091\n",
      "Epoch [50/200], Loss: 0.3707\n",
      "Epoch [50/200], Loss: 0.4104\n",
      "Epoch [60/200], Loss: 0.5408\n",
      "Epoch [60/200], Loss: 0.5406\n",
      "Epoch [60/200], Loss: 0.4589\n",
      "Epoch [60/200], Loss: 0.5211\n",
      "Epoch [60/200], Loss: 0.3979\n",
      "Epoch [60/200], Loss: 0.3848\n",
      "Epoch [60/200], Loss: 0.3683\n",
      "Epoch [60/200], Loss: 0.3669\n",
      "Epoch [60/200], Loss: 0.5344\n",
      "Epoch [60/200], Loss: 0.5915\n",
      "Epoch [60/200], Loss: 0.4278\n",
      "Epoch [60/200], Loss: 0.4725\n",
      "Epoch [60/200], Loss: 0.4274\n",
      "Epoch [70/200], Loss: 0.3992\n",
      "Epoch [70/200], Loss: 0.5409\n",
      "Epoch [70/200], Loss: 0.3886\n",
      "Epoch [70/200], Loss: 0.4630\n",
      "Epoch [70/200], Loss: 0.5588\n",
      "Epoch [70/200], Loss: 0.5228\n",
      "Epoch [70/200], Loss: 0.3884\n",
      "Epoch [70/200], Loss: 0.3191\n",
      "Epoch [70/200], Loss: 0.4941\n",
      "Epoch [70/200], Loss: 0.5055\n",
      "Epoch [70/200], Loss: 0.4882\n",
      "Epoch [70/200], Loss: 0.4584\n",
      "Epoch [70/200], Loss: 0.4203\n",
      "Epoch [80/200], Loss: 0.5119\n",
      "Epoch [80/200], Loss: 0.4948\n",
      "Epoch [80/200], Loss: 0.3950\n",
      "Epoch [80/200], Loss: 0.4857\n",
      "Epoch [80/200], Loss: 0.5401\n",
      "Epoch [80/200], Loss: 0.4722\n",
      "Epoch [80/200], Loss: 0.4145\n",
      "Epoch [80/200], Loss: 0.5182\n",
      "Epoch [80/200], Loss: 0.3222\n",
      "Epoch [80/200], Loss: 0.4871\n",
      "Epoch [80/200], Loss: 0.4423\n",
      "Epoch [80/200], Loss: 0.3900\n",
      "Epoch [80/200], Loss: 0.3880\n",
      "Epoch [90/200], Loss: 0.4499\n",
      "Epoch [90/200], Loss: 0.4390\n",
      "Epoch [90/200], Loss: 0.3566\n",
      "Epoch [90/200], Loss: 0.4616\n",
      "Epoch [90/200], Loss: 0.4878\n",
      "Epoch [90/200], Loss: 0.4918\n",
      "Epoch [90/200], Loss: 0.4846\n",
      "Epoch [90/200], Loss: 0.3853\n",
      "Epoch [90/200], Loss: 0.4645\n",
      "Epoch [90/200], Loss: 0.4640\n",
      "Epoch [90/200], Loss: 0.5435\n",
      "Epoch [90/200], Loss: 0.4040\n",
      "Epoch [90/200], Loss: 0.3380\n",
      "Epoch [100/200], Loss: 0.4797\n",
      "Epoch [100/200], Loss: 0.3680\n",
      "Epoch [100/200], Loss: 0.4186\n",
      "Epoch [100/200], Loss: 0.4108\n",
      "Epoch [100/200], Loss: 0.4299\n",
      "Epoch [100/200], Loss: 0.4697\n",
      "Epoch [100/200], Loss: 0.4116\n",
      "Epoch [100/200], Loss: 0.4123\n",
      "Epoch [100/200], Loss: 0.4375\n",
      "Epoch [100/200], Loss: 0.3697\n",
      "Epoch [100/200], Loss: 0.5632\n",
      "Epoch [100/200], Loss: 0.4541\n",
      "Epoch [100/200], Loss: 0.6332\n",
      "Epoch [110/200], Loss: 0.4145\n",
      "Epoch [110/200], Loss: 0.4593\n",
      "Epoch [110/200], Loss: 0.4648\n",
      "Epoch [110/200], Loss: 0.4561\n",
      "Epoch [110/200], Loss: 0.5023\n",
      "Epoch [110/200], Loss: 0.4247\n",
      "Epoch [110/200], Loss: 0.4720\n",
      "Epoch [110/200], Loss: 0.4536\n",
      "Epoch [110/200], Loss: 0.4283\n",
      "Epoch [110/200], Loss: 0.3842\n",
      "Epoch [110/200], Loss: 0.4450\n",
      "Epoch [110/200], Loss: 0.3878\n",
      "Epoch [110/200], Loss: 0.3716\n",
      "Epoch [120/200], Loss: 0.3725\n",
      "Epoch [120/200], Loss: 0.3727\n",
      "Epoch [120/200], Loss: 0.4931\n",
      "Epoch [120/200], Loss: 0.4561\n",
      "Epoch [120/200], Loss: 0.4093\n",
      "Epoch [120/200], Loss: 0.4589\n",
      "Epoch [120/200], Loss: 0.4126\n",
      "Epoch [120/200], Loss: 0.4482\n",
      "Epoch [120/200], Loss: 0.5132\n",
      "Epoch [120/200], Loss: 0.4359\n",
      "Epoch [120/200], Loss: 0.4677\n",
      "Epoch [120/200], Loss: 0.3139\n",
      "Epoch [120/200], Loss: 0.5400\n",
      "Epoch [130/200], Loss: 0.4310\n",
      "Epoch [130/200], Loss: 0.2684\n",
      "Epoch [130/200], Loss: 0.4036\n",
      "Epoch [130/200], Loss: 0.3853\n",
      "Epoch [130/200], Loss: 0.4549\n",
      "Epoch [130/200], Loss: 0.5257\n",
      "Epoch [130/200], Loss: 0.5064\n",
      "Epoch [130/200], Loss: 0.4190\n",
      "Epoch [130/200], Loss: 0.4571\n",
      "Epoch [130/200], Loss: 0.4630\n",
      "Epoch [130/200], Loss: 0.4136\n",
      "Epoch [130/200], Loss: 0.4202\n",
      "Epoch [130/200], Loss: 0.4614\n",
      "Epoch [140/200], Loss: 0.3372\n",
      "Epoch [140/200], Loss: 0.3552\n",
      "Epoch [140/200], Loss: 0.4671\n",
      "Epoch [140/200], Loss: 0.3748\n",
      "Epoch [140/200], Loss: 0.5233\n",
      "Epoch [140/200], Loss: 0.4626\n",
      "Epoch [140/200], Loss: 0.4815\n",
      "Epoch [140/200], Loss: 0.3967\n",
      "Epoch [140/200], Loss: 0.3582\n",
      "Epoch [140/200], Loss: 0.4498\n",
      "Epoch [140/200], Loss: 0.5034\n",
      "Epoch [140/200], Loss: 0.4169\n",
      "Epoch [140/200], Loss: 0.4168\n",
      "Epoch [150/200], Loss: 0.3642\n",
      "Epoch [150/200], Loss: 0.3698\n",
      "Epoch [150/200], Loss: 0.4023\n",
      "Epoch [150/200], Loss: 0.4628\n",
      "Epoch [150/200], Loss: 0.3980\n",
      "Epoch [150/200], Loss: 0.5951\n",
      "Epoch [150/200], Loss: 0.3621\n",
      "Epoch [150/200], Loss: 0.4538\n",
      "Epoch [150/200], Loss: 0.4384\n",
      "Epoch [150/200], Loss: 0.3754\n",
      "Epoch [150/200], Loss: 0.4963\n",
      "Epoch [150/200], Loss: 0.3620\n",
      "Epoch [150/200], Loss: 0.4397\n",
      "Epoch [160/200], Loss: 0.3443\n",
      "Epoch [160/200], Loss: 0.3970\n",
      "Epoch [160/200], Loss: 0.4916\n",
      "Epoch [160/200], Loss: 0.4513\n",
      "Epoch [160/200], Loss: 0.3492\n",
      "Epoch [160/200], Loss: 0.4487\n",
      "Epoch [160/200], Loss: 0.3939\n",
      "Epoch [160/200], Loss: 0.4263\n",
      "Epoch [160/200], Loss: 0.3703\n",
      "Epoch [160/200], Loss: 0.4304\n",
      "Epoch [160/200], Loss: 0.5073\n",
      "Epoch [160/200], Loss: 0.4016\n",
      "Epoch [160/200], Loss: 0.4953\n",
      "Epoch [170/200], Loss: 0.4581\n",
      "Epoch [170/200], Loss: 0.3710\n",
      "Epoch [170/200], Loss: 0.4144\n",
      "Epoch [170/200], Loss: 0.4447\n",
      "Epoch [170/200], Loss: 0.4344\n",
      "Epoch [170/200], Loss: 0.4126\n",
      "Epoch [170/200], Loss: 0.3961\n",
      "Epoch [170/200], Loss: 0.3959\n",
      "Epoch [170/200], Loss: 0.3892\n",
      "Epoch [170/200], Loss: 0.4128\n",
      "Epoch [170/200], Loss: 0.4531\n",
      "Epoch [170/200], Loss: 0.3850\n",
      "Epoch [170/200], Loss: 0.5103\n",
      "Epoch [180/200], Loss: 0.3736\n",
      "Epoch [180/200], Loss: 0.3833\n",
      "Epoch [180/200], Loss: 0.3432\n",
      "Epoch [180/200], Loss: 0.3920\n",
      "Epoch [180/200], Loss: 0.4793\n",
      "Epoch [180/200], Loss: 0.3875\n",
      "Epoch [180/200], Loss: 0.3043\n",
      "Epoch [180/200], Loss: 0.4928\n",
      "Epoch [180/200], Loss: 0.3672\n",
      "Epoch [180/200], Loss: 0.4557\n",
      "Epoch [180/200], Loss: 0.5428\n",
      "Epoch [180/200], Loss: 0.4737\n",
      "Epoch [180/200], Loss: 0.3911\n",
      "Epoch [190/200], Loss: 0.4365\n",
      "Epoch [190/200], Loss: 0.4691\n",
      "Epoch [190/200], Loss: 0.3331\n",
      "Epoch [190/200], Loss: 0.4521\n",
      "Epoch [190/200], Loss: 0.3927\n",
      "Epoch [190/200], Loss: 0.4541\n",
      "Epoch [190/200], Loss: 0.4165\n",
      "Epoch [190/200], Loss: 0.4022\n",
      "Epoch [190/200], Loss: 0.4493\n",
      "Epoch [190/200], Loss: 0.3724\n",
      "Epoch [190/200], Loss: 0.3210\n",
      "Epoch [190/200], Loss: 0.4685\n",
      "Epoch [190/200], Loss: 0.3858\n",
      "Epoch [200/200], Loss: 0.4497\n",
      "Epoch [200/200], Loss: 0.4881\n",
      "Epoch [200/200], Loss: 0.4449\n",
      "Epoch [200/200], Loss: 0.4092\n",
      "Epoch [200/200], Loss: 0.3489\n",
      "Epoch [200/200], Loss: 0.3509\n",
      "Epoch [200/200], Loss: 0.3833\n",
      "Epoch [200/200], Loss: 0.4364\n",
      "Epoch [200/200], Loss: 0.3735\n",
      "Epoch [200/200], Loss: 0.4205\n",
      "Epoch [200/200], Loss: 0.4392\n",
      "Epoch [200/200], Loss: 0.4452\n",
      "Epoch [200/200], Loss: 0.2663\n",
      "\n",
      "Accuracy: 0.81125\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Evaluate the model's accuracy on the training set\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy:', accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.755\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy on the testing set\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
