{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efa18ec-0d22-40b8-87a3-b2b5406584bb",
   "metadata": {},
   "source": [
    "# A Simple Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6d3ac-8941-4d4d-97c4-b0c25f9880c2",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore how to build a simple neural network model using PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc841a-f938-4ffa-8066-c72528319ad2",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003a77ed-fc2c-42cd-a5d7-88dc4ccf046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4946ca0-5548-44f5-8eb3-5d8acb110584",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e9939-7b20-450b-8b83-2d820e6a8c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/yangliuiuk/data/main/diabetes.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc69a32-aba5-49a7-880f-340621bb3032",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aae826-7178-46cf-a0b0-e59cfc7a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype('float32'))\n",
    "X_test_tensor = torch.tensor(X_test.astype('float32'))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype('float32')).unsqueeze(1) \n",
    "# Using y_train.values is necessary because PyTorch tensors expect NumPy arrays as input, not pandas Series objects.\n",
    "# X_train has already been converted to Numpy array by the scaler\n",
    "y_test_tensor = torch.tensor(y_test.values.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b51f6f-7583-4861-892d-fc07febd5f37",
   "metadata": {},
   "source": [
    "### Step 4: Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa86025-ad51-49bc-8ae3-12d3f1005bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)  # Input layer to hidden layer\n",
    "        self.fc2 = nn.Linear(16, 8)           # Hidden layer to hidden layer\n",
    "        self.fc3 = nn.Linear(8, 1)            # Hidden layer to output layer\n",
    "        self.relu = nn.ReLU()                  # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))  # Pass through first hidden layer\n",
    "        x = self.relu(self.fc2(x))  # Pass through second hidden layer\n",
    "        x = torch.sigmoid(self.fc3(x))  # Pass through output layer with sigmoid activation\n",
    "        return x\n",
    "    \n",
    "    #def forward(self, x):\n",
    "    #    h1 = nm.Linear(input_size, 16)(x)\n",
    "    #    h1 = nm.ReLu(h1)\n",
    "\n",
    "    #    h2 = nm.Linear(16, 16)(h1)\n",
    "    #    h2 = nm.ReLu(h2)\n",
    "\n",
    "    #    h3 = nm.Linear(16,1)(h2)\n",
    "\n",
    "    #    y = torch.sigmoid(h3)\n",
    "\n",
    "    #   return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f39121-f228-4d67-a7a4-a93bfd26da1a",
   "metadata": {},
   "source": [
    "In PyTorch, **nn.Linear** is a class that represents a linear transformation layer. It is commonly used to define the linear mapping between the input and output layers of a neural network.\n",
    "\n",
    "The nn.Linear layer performs a linear transformation on the input data by multiplying it with a weight matrix and adding a bias term. Mathematically, the output of nn.Linear layer can be represented as:\n",
    "\n",
    "output = input Ã— weight + bias\n",
    "\n",
    "Here's a brief explanation of the parameters:\n",
    "\n",
    "- in_features: The number of input features or dimensions.\n",
    "- out_features: The number of output features or dimensions.\n",
    "- bias: A boolean parameter indicating whether to include a bias term in the linear transformation. If set to True, a bias vector will be added to the output.\n",
    "\n",
    "When you create an instance of nn.Linear, you specify the input and output dimensions. For example, nn.Linear(10, 5) creates a linear layer with 10 input features and 5 output features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b97c88-feef-460d-ac04-08bb31771936",
   "metadata": {},
   "source": [
    "In PyTorch, **nn.ReLU** is a class representing the Rectified Linear Unit (ReLU) activation function. It's one of the most commonly used activation functions in deep learning models.\n",
    "\n",
    "The ReLU function is defined as:\n",
    "\n",
    "f(x)=max(0,x)\n",
    "\n",
    "It simply replaces any negative input with zero, while leaving positive values unchanged. Geometrically, it looks like a ramp starting from the origin.\n",
    "\n",
    "In the context of neural networks, nn.ReLU is applied element-wise to the output of a linear transformation (or any other layer) to introduce non-linearity into the model. This non-linearity is crucial for the network to learn complex patterns and relationships in the data.\n",
    "\n",
    "Here's a brief overview of how nn.ReLU works:\n",
    "\n",
    "- For each element in the input tensor, if the element is less than zero, it is replaced with zero. Otherwise, it remains unchanged.\n",
    "- The operation is performed independently on each element of the input tensor.\n",
    "- The output tensor has the same shape as the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098a7f8-d2b5-4e37-aa94-f697a1e71283",
   "metadata": {},
   "source": [
    "### Step 5: Initialize the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29195fb6-08b2-4f24-aedf-d4420ffc3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural network, creating a new instance of the neural network model.\n",
    "input_size = X_train_tensor.shape[1]  # Number of features\n",
    "model = NeuralNetwork(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f975a1f-242f-4d02-8e17-ee844325b1fe",
   "metadata": {},
   "source": [
    "### Step 6: Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d89febb-0493-4695-9a5b-827a3d89abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1af6c7-dab5-4ce7-b129-fadc7f17ea58",
   "metadata": {},
   "source": [
    "### Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1b278d-66f7-4a76-b467-77d720df2c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.5447\n",
      "Epoch [200/1000], Loss: 0.4372\n",
      "Epoch [300/1000], Loss: 0.4192\n",
      "Epoch [400/1000], Loss: 0.4021\n",
      "Epoch [500/1000], Loss: 0.3857\n",
      "Epoch [600/1000], Loss: 0.3708\n",
      "Epoch [700/1000], Loss: 0.3593\n",
      "Epoch [800/1000], Loss: 0.3480\n",
      "Epoch [900/1000], Loss: 0.3369\n",
      "Epoch [1000/1000], Loss: 0.3274\n",
      "Accuracy on training set: 0.8632\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train_tensor)\n",
    "    predicted = (outputs >= 0.5).float()\n",
    "    accuracy = (predicted == y_train_tensor).sum().item() / len(y_train_tensor)\n",
    "    print(f'Accuracy on training set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14161165-43d3-48a0-a29f-49c9ced71b62",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745f66e5-e289-4f7e-80c0-3656f0194e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predicted = (outputs >= 0.5).float()\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba70fe0-c524-4823-b1f8-2a6776c07c27",
   "metadata": {},
   "source": [
    "### Step 9: Tuning the Neural Network\n",
    "By comparing the accuracy on test set (0.7078) and the accuracy on the training set (0.8567), we can see there is an overfitting problem. This is due to the neural network model is too complex so it is prone to overfitting. There are many methods to address this issue, including:\n",
    "\n",
    "Increase the Size of the Training Data: Providing more diverse and representative data to the model can help it learn the underlying patterns better and reduce overfitti\n",
    "\n",
    "Early Stop: It involves monitoring the performance of the model on a separate validation dataset during training and stopping the training process when the performance of the model starts to degrade on the validation set.ng.\n",
    "\n",
    "Regularization Techniques:\n",
    "\n",
    "L2 Regularization (Weight Decay): Add a penalty term to the loss function that penalizes large weights, preventing them from becoming too large and causing overfitting.\n",
    "\n",
    "Dropout: Randomly drop neurons during training to prevent them from relying too much on each other and thus reduce overfitting. \n",
    "\n",
    "Batch Normalization: Normalize the activations of each layer to reduce internal covariate shift, which can help stabilize and regularize the training process.\n",
    "\n",
    "Reduce Model Complexity:\n",
    "\n",
    "Decrease the Number of Parameters: Reduce the number of layers or neurons in the network to decrease its capacity and make it less prone to overfitting.\n",
    "\n",
    "Simplify the Architecture: Use simpler architectures or techniques like early stopping to prevent the model from learning overly complex representations.\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to evaluate the model's performance on multiple validation sets and obtain a more robust estimate of its generalization performance.\n",
    "\n",
    "Data Augmentation: Generate additional training data by applying transformations like rotation, scaling, or cropping to the existing data, which can help the model generalize better.\n",
    "\n",
    "Ensemble Learning: Train multiple models with different architectures or random initializations and combine their predictions to improve generalization performance.\n",
    "\n",
    "Hyperparameter Tuning: Experiment with different hyperparameters such as learning rate, batch size, and optimizer to find the optimal configuration that reduces overfitting.at reduces overfitting.\n",
    "uces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1ebab-901f-4b5b-b741-8fe1d8457555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dca292-20a7-4fe3-aa4d-fe0ebd055f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
