{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text Classfier\n",
    "### What is Text Classification?\n",
    "Text classification, also known as text categorization, is the task of assigning predefined categories or labels to a document based on its content. It involves automatically classifying a piece of text into one or more predefined categories or classes. Text classification is a fundamental task in natural language processing (NLP) and is used in various applications such as spam detection, sentiment analysis, topic classification, and document categorization.\n",
    "### How to Represent a Text Document?\n",
    "There are several common methods for representing a text document in a format that can be processed by machine learning algorithms. Here are some of the most popular techniques:\n",
    "\n",
    "- Bag-of-Words (BoW): In this approach, each document is represented as a vector where each element corresponds to the frequency of a particular word in the document. Stop words (commonly occurring words like \"and,\" \"the,\" \"is\") are often removed to reduce noise. The order of words is disregarded, hence the term \"bag-of-words.\"\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF): TF-IDF is similar to the BoW approach but also takes into account the importance of words in the context of the entire corpus. It calculates a weight for each word based on its frequency in the document (term frequency) and its rarity across all documents (inverse document frequency).\n",
    "\n",
    "- Word Embeddings: Word embeddings represent words as dense, low-dimensional vectors in a continuous vector space. Techniques like Word2Vec, GloVe, and FastText learn to map words to these vector representations in a way that captures semantic relationships between words.\n",
    "\n",
    "- N-grams: Instead of considering individual words, N-grams represent sequences of N contiguous words in the text. For example, bigrams (N=2) consider pairs of consecutive words, while trigrams (N=3) consider sequences of three words.\n",
    "\n",
    "- Character-level Representations: In some cases, especially when dealing with languages with complex morphology or when word boundaries are not well-defined (e.g., Chinese or Thai), character-level representations can be used. Each character is treated as a separate feature.\n",
    "\n",
    "In this tutorial, we will build a simple text classifier based on Bag-of-Words representation of document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "In this tutorial, we will use the IMDB review dataset. \n",
    "\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
    "\n",
    "The dataset can be accessed from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sentiment labels to numerical values (0 for negative, 1 for positive)\n",
    "df['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features (reviews) and target (sentiment)\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert each text document (movie review) into a \"bag of words\", i.e., a vector of term frequencies (TF). \n",
    "\"Term\", \"token\", and \"word\" refer to the same concept. \n",
    "By vectorization, we can convert a collection of text documents into a matrix of token counts.\n",
    "The \"CountVectorizer\" class in sklearn library can be used for this task. Here's an explanation of how it works:\n",
    "\n",
    "- **Tokenization**: First, the text data is tokenized, which means splitting the text into individual words or terms. For example, the sentence \"The quick brown fox jumps over the lazy dog\" would be tokenized into [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]. \n",
    "- **Stopword removal**: Stopwords like \"a\", \"the\" don't have actually meanings so are often removed from the token list to reduce noise.\n",
    "- **Vocabulary Building**: Next, the unique tokens from all the documents are collected to build a vocabulary. This vocabulary consists of all the unique words or terms present in the entire corpus of documents.\n",
    "- **Vectorization**: For each document, the count of each token in the vocabulary is calculated. This count represents how many times each token appears in the document. The result is a matrix where each row corresponds to a document and each column corresponds to a token in the vocabulary. The value at each position in the matrix is the count of the corresponding token in the document.\n",
    "\n",
    "For example, consider the following documents:\n",
    "\n",
    "- Document 1: \"The quick brown fox\"\n",
    "- Document 2: \"The lazy dog\"\n",
    "\n",
    "The vocabulary would be: [\"The\", \"quick\", \"brown\", \"fox\", \"lazy\", \"dog\"]\n",
    "\n",
    "The count matrix would be:\n",
    "\n",
    "            The  quick  brown  fox  lazy  dog\n",
    "            \n",
    "Document 1:   1     1      1     1     0    0\n",
    "\n",
    "Document 2:   1     0      0     0     1    1\n",
    "\n",
    "Each row in the matrix represents a document, and each column represents a token from the vocabulary. The value at each position indicates the count of the corresponding token in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical features using CountVectorizer\n",
    "#vectorizer = TfidfVectorizer(stop_words= 'english')\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fit_transform** method is used to both fit the vectorizer to the training data and transform the training data into a matrix of token counts or TF-IDF features. \n",
    "During the fitting process, the vectorizer learns the vocabulary from the training data, which involves tokenization and vocabulary building. It collects all unique tokens (words or terms) from the training data and assigns each one an index.\n",
    "\n",
    "The **transform** method is used to transform the testing or unseen data into the same matrix representation based on the vocabulary learned from the training data.\n",
    "Unlike fit_transform, transform does not learn the vocabulary or build the token count matrix. Instead, it only applies the transformation based on the existing vocabulary learned during the fitting process.\n",
    "This method is applied to the testing or new data after the vectorizer has been fitted to the training data. It ensures that the testing data is represented in the same format as the training data, allowing for consistency in feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 92692)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 92692)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 92692]) torch.Size([10000, 92692]) torch.Size([40000]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "# try using Data loader\n",
    "X_train = torch.tensor(X_train_vec, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_vec, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)  # Output layer with 2 neurons for binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = X_train.shape[1]\n",
    "model = Net(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/1250], Loss: 0.4648\n",
      "Epoch [1/1], Step [200/1250], Loss: 0.3297\n",
      "Epoch [1/1], Step [300/1250], Loss: 0.3693\n",
      "Epoch [1/1], Step [400/1250], Loss: 0.3288\n",
      "Epoch [1/1], Step [500/1250], Loss: 0.3067\n",
      "Epoch [1/1], Step [600/1250], Loss: 0.3007\n",
      "Epoch [1/1], Step [700/1250], Loss: 0.3039\n",
      "Epoch [1/1], Step [800/1250], Loss: 0.2859\n",
      "Epoch [1/1], Step [900/1250], Loss: 0.2967\n",
      "Epoch [1/1], Step [1000/1250], Loss: 0.2839\n",
      "Epoch [1/1], Step [1100/1250], Loss: 0.2750\n",
      "Epoch [1/1], Step [1200/1250], Loss: 0.2644\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, targets = data\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 89.62%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, targets = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.62%\n"
     ]
    }
   ],
   "source": [
    "# Alternative\n",
    "# Evaluation on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f'Test Accuracy: {100 * accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
